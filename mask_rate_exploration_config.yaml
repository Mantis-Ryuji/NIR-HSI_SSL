seed: 42

chemomae:
  seq_len: 256
  d_model: 512
  nhead: 8
  num_layers: 8
  dim_feedforward: 2048
  dropout: 0.1
  decoder_num_layers: 1
  latent_dim: 16
  n_patches: 16

training:
  dataLoader:
    batch_size: 4096         
    num_workers: 8           
    pin_memory: true         
    persistent_workers: true 

  optimizer:
    base_lr: 5e-4
    weight_decay: 1e-3 
    betas: [0.9, 0.95] 
    eps: 1.0e-8 

  scheduler: # warmup + cosine annealing
    warmup_epochs: 2 
    min_lr_scale: 0.02      # 最終LRは base_lr×0.02。
    
  trainer:
    config:
      # out_dir: f"runs/mask_rate_exploration/n_mask{int(n_mask)}/mae"下に weight を保存"
      device: "cuda"
      amp: True
      amp_dtype: "bf16"
      enable_tf32: False
      grad_clip: 1.0
      use_ema: True
      ema_decay: 0.999
      loss_type: "sse"
      reduction: "batch_mean"
      early_stop_patience: 20
      early_stop_start_ratio: 0.5
      early_stop_min_delta: 0.0
      resume_from: "auto"
    fit:
      epochs: 200

evaluation:
  mlpconfig:
    # runs/mask_rate_exploration/n_mask{int(n_mask)}/eval/each_task下に weight, result を保存
    width: 256
    depth: 2
    dropout: float = 0.1
    activation: "gelu"
    norm: "layernorm"
    residual: True

    # training
    epochs: 20
    lr: 5e-4
    weight_decay: 1e-4
    optimizer: adamw"
    grad_clip_norm: 1.0

    # compute
    device: "cuda"
    amp: False

    # early stopping
    early_stopping: True
    patience: 5
    min_delta: 0.0
    restore_best: True